{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this Jupyter Notebook is sourced from the following web page: [GitHub - Lora for sequence classification with Roberta-Llama-Mistral](https://github.com/mehdiir/Roberta-Llama-Mistral/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512 \n",
    "roberta_checkpoint = \"roberta-large\"\n",
    "mistral_checkpoint = \"mistralai/Mistral-7B-v0.1\"\n",
    "llama_checkpoint = \"meta-llama/Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation\n",
    "\n",
    "### Read in the training data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2236 entries, 0 to 2235\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   search_phrase    2236 non-null   object \n",
      " 1   label            2236 non-null   int64  \n",
      " 2   email            2236 non-null   object \n",
      " 3   mistral_pred     2236 non-null   float64\n",
      " 4   openhermes_pred  2236 non-null   float64\n",
      " 5   vicuna_pred      2236 non-null   float64\n",
      " 6   gemma_pred       2236 non-null   float64\n",
      "dtypes: float64(4), int64(1), object(2)\n",
      "memory usage: 122.4+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 559 entries, 0 to 558\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   search_phrase    559 non-null    object \n",
      " 1   label            559 non-null    int64  \n",
      " 2   email            559 non-null    object \n",
      " 3   mistral_pred     559 non-null    float64\n",
      " 4   openhermes_pred  559 non-null    float64\n",
      " 5   vicuna_pred      559 non-null    float64\n",
      " 6   gemma_pred       559 non-null    float64\n",
      " 7   target           559 non-null    int64  \n",
      "dtypes: float64(4), int64(2), object(2)\n",
      "memory usage: 35.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "DATA_PATH = \"../data/\"\n",
    "train_df=pd.read_csv(os.path.join(DATA_PATH, 'enron_labeled_curated_train.csv'))\n",
    "test_df=pd.read_csv(os.path.join(DATA_PATH, 'enron_labeled_curated_test.csv'))\n",
    "# dummy target column for merge test and train into one huggingface data\n",
    "test_df['target'] = 0 \n",
    "train_df.info()\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the classes are not balanced, we will compute the positive and negative weights and use them for loss calculation later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    1918\n",
      "1     318\n",
      "Name: count, dtype: int64\n",
      "3.5157232704402515 0.5828988529718456\n"
     ]
    }
   ],
   "source": [
    "print(train_df.label.value_counts())\n",
    "pos_weights = len(train_df) / (2 * train_df.label.value_counts()[1])\n",
    "neg_weights = len(train_df) / (2 * train_df.label.value_counts()[0])\n",
    "POS_WEIGHT, NEG_WEIGHT = (pos_weights, neg_weights)\n",
    "print(POS_WEIGHT, NEG_WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of character is 2983.\n",
      "The maximum number of word is 650.\n"
     ]
    }
   ],
   "source": [
    "##Then, we compute the maximum length of the column text:\n",
    "# Number of Characters\n",
    "max_char=train_df['email'].str.len().max()\n",
    "# Number of Words\n",
    "max_words = train_df['email'].str.split().str.len().max()\n",
    "print(f\"The maximum number of character is {max_char}.\")\n",
    "print(f\"The maximum number of word is {max_words}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/External/opt/anaconda3/envs/ollama2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1788/1788 [00:00<00:00, 93701.70 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 448/448 [00:00<00:00, 59909.08 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 559/559 [00:00<00:00, 128591.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#Now, let's convert the dataframe into HuggingFace dataset format, \n",
    "#split it into training and validation datasets, add the test dataset and save the three files:\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "#Convert the training dataframe to HuggingFace dataset\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "#Split the dataset into training and validation datasets\n",
    "data = dataset.train_test_split(train_size=0.8, seed=42)\n",
    "#Rename the default \"test\" split to \"validation\"\n",
    "data['val'] = data.pop(\"test\")\n",
    "#Convert the test dataframe to HuggingFace dataset and add it into the first dataset\n",
    "data['test'] = Dataset.from_pandas(test_df)\n",
    "#Save\n",
    "data.save_to_disk(\"processed_hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comprises a keyword, a location and the text of the tweet. For sake of simplicity, we select the text feature only as the input to the LLM.\n",
    "\n",
    "At this stage, we prepared the train, validation, and test sets in the HuggingFace format expected by the pre-trained LLMs. The next step is to define the tokenized dataset for training using the appropriate tokenizer to transform the text feature into two Tensors of sequence of token ids and attention masks. As each model has its specific tokenizer, we will need to define three different datasets.\n",
    "\n",
    "We start by defining the RoBERTa dataloader:\n",
    "\n",
    "Load the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 520, 327, 9, 2271, 2839, 12662, 31, 68, 6232, 656, 42, 76, 7, 15655, 4276, 5214, 6, 1583, 9, 5, 138, 18, 1321, 685, 45, 95, 49, 1315, 53, 67, 7458, 5214, 620, 9, 5, 923, 9, 49, 17936, 1640, 330, 43, 3832, 2349, 4, 286, 5, 674, 12735, 5214, 1942, 6, 2271, 2839, 388, 4625, 130, 12, 41830, 15561, 9, 17936, 1640, 330, 43, 1781, 6, 8, 5, 1007, 740, 5214, 7474, 3785, 18, 24053, 480, 71, 14613, 9, 12030, 6, 1153, 15381, 6, 5457, 36617, 154, 3464, 480, 21, 10, 1081, 28857, 1571, 4, 5214, 844, 5975, 3770, 6, 217, 1557, 12274, 4, 4810, 7803, 254, 9, 886, 8, 5214, 4160, 2812, 329, 833, 9, 188, 3123, 6, 32, 6404, 11, 7, 1744, 97, 1791, 31, 5457, 42116, 12884, 4, 178, 270, 3516, 34, 2740, 22, 102, 714, 1551, 7, 27067, 5214, 3894, 82, 18, 15131, 72, 125, 168, 6530, 74, 129, 6581, 10, 385, 5214, 8395, 1827, 1114, 35, 14, 867, 4395, 75, 4649, 5, 6976, 9, 49, 308, 5044, 8539, 5214, 1790, 4, 7939, 18, 28, 699, 35, 2271, 2839, 4585, 8, 751, 9818, 9314, 54, 15005, 7, 867, 5214, 480, 217, 49, 308, 733, 6, 151, 1321, 480, 4649, 41, 7934, 2640, 5214, 6, 7654, 8, 1030, 4, 125, 5, 1321, 67, 4649, 103, 2640, 4, 1993, 9, 5214, 106, 56, 444, 350, 739, 10, 10301, 9, 49, 3832, 1781, 3016, 62, 11, 5457, 25017, 308, 138, 4, 252, 362, 10, 810, 4, 5214, 844, 16040, 2839, 1661, 10, 6097, 17936, 1640, 330, 3256, 13479, 115, 3754, 62, 7, 231, 207, 9, 49, 17279, 5214, 1090, 582, 11, 10, 1810, 1186, 9, 1735, 6, 217, 388, 7628, 1188, 101, 30424, 118, 5214, 2553, 28387, 30351, 6, 3554, 6, 418, 12, 2989, 1188, 8, 10, 1403, 12, 25706, 14376, 873, 1316, 326, 5214, 8849, 115, 907, 15655, 932, 4, 252, 115, 67, 2807, 2271, 2839, 388, 6, 9695, 5214, 611, 11835, 23, 5, 1675, 210, 425, 4, 11999, 1321, 3162, 19, 5, 5214, 853, 308, 418, 6, 5, 138, 9184, 6, 62, 7, 654, 207, 36, 6025, 16, 6, 155, 207, 9, 1542, 582, 238, 22094, 5214, 298, 2271, 2839, 388, 4, 5214, 844, 1121, 97, 1617, 6, 481, 2271, 2839, 388, 480, 68, 134, 6, 3913, 966, 10, 76, 13, 41, 3200, 475, 677, 5214, 154, 68, 2466, 6, 151, 11, 1542, 582, 480, 21, 233, 9, 5, 4660, 3737, 4, 10586, 11269, 5214, 2753, 24, 6, 8, 51, 15546, 6640, 24, 4, 10586, 67, 1467, 14, 49, 17936, 1640, 330, 43, 2489, 5214, 417, 10, 2178, 6, 67, 1537, 7, 215, 708, 6, 14, 51, 56, 7, 489, 5, 138, 1690, 1975, 5214, 330, 576, 7, 106, 30, 2271, 2839, 454, 51, 58, 23, 513, 654, 107, 793, 4, 5053, 2271, 2839, 579, 5214, 90, 3343, 51, 2162, 1235, 6, 9, 768, 6, 51, 115, 2937, 23, 40, 4, 5214, 844, 16040, 2839, 388, 14622, 11, 923, 480, 53, 1224, 66, 7, 28, 10, 14164, 8, 10, 26020, 4, 5214, 15467, 5, 403, 9, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_checkpoint, add_prefix_space=True)\n",
    "\n",
    "\n",
    "#Define the preprocessing function for converting one row of the dataframe:\n",
    "def roberta_preprocessing_function(examples):\n",
    "    return roberta_tokenizer(examples['email'], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "#By applying the preprocessing function to the first example of our training dataset, we can see that we have inputs ids and an attention mask:\n",
    "roberta_preprocessing_function(data['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'search_phrase': 'it appears that some Enron employees used dummy accounts and rigged valuation methodologies to create false profit-and-loss entries for the derivatives',\n",
       " 'label': 0,\n",
       " 'email': 'When shares of Enron plunged from $84 earlier this year to practically zero=, thousands of the company\\'s employees lost not just their jobs but also mo=st of the value of their 401(k) retirement accounts. For the average employ=ee, Enron stock represented three-fifths of 401(k) assets, and the energy c=ompany\\'s meltdown -- after revelations of misleading, probably fraudulent, =accounting practices -- was a personal calamity.=20Now politicians, including Democratic Sens. Barbara Boxer of California and= Jon Corzine of New Jersey, are rushing in to protect other Americans from =similar disasters. And President Bush has ordered \"a policy review to prote=ct people\\'s pensions.\" But government intervention would only introduce a d=angerous idea: that investors shouldn\\'t bear the burden of their own decisi=ons.Let\\'s be clear: Enron executives and outside auditors who lied to investors= -- including their own 21,000 employees -- bear an enormous responsibility=, moral and legal. But the employees also bear some responsibility. Most of= them had far too large a proportion of their retirement assets tied up in =their own company. They took a risk.=20Enron offered a typical 401(k): Employees could invest up to 6% of their ba=se pay in a wide range of options, including stock mutual funds like Fideli=ty Magellan, bonds, money-market funds and a self-directed Schwab account t=hat could buy practically anything. They could also choose Enron stock, pur=chased at the regular market price. Whatever employees contributed with the=ir own money, the company matched, up to 50% (that is, 3% of base pay), wit=h Enron stock.=20In other words, free Enron stock -- $1,800 worth a year for an employee mak=ing $60,000 in base pay -- was part of the compensation package. Workers kn=ew it, and they presumably liked it. Workers also knew that their 401(k) ha=d a rule, also common to such plans, that they had to keep the company stoc=k given to them by Enron until they were at least 50 years old. Any Enron s=tock they bought themselves, of course, they could transfer at will.=20Enron stock soared in value -- but turned out to be a blessing and a curse.= Imagine the case of an employee who joined the company in 1997, when the s=tock was worth (after adjusting for splits) about $20 a share. If the emplo=yee bought other assets which grew at 10% a year, by the end of 2000 those =assets had grown by about one-third while the Enron stock had more than qua=drupled. His 401(k) account became lopsided, with far more Enron stock than= anything else.=20That was a predicament common to many Enron employees when their company st=ock crashed. The wise move, as Enron climbed, would have been to buy other =assets for a separate, taxable plan to balance the company stock. How many =employees did that? Probably not many -- presumably for some because they d=idn\\'t have the money. If Social Security had been reformed, they could have=',\n",
       " 'mistral_pred': 0.0,\n",
       " 'openhermes_pred': 0.0,\n",
       " 'vicuna_pred': 0.7,\n",
       " 'gemma_pred': 0.6}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1788/1788 [00:01<00:00, 992.98 examples/s] \n",
      "Map: 100%|██████████| 448/448 [00:00<00:00, 856.64 examples/s] \n",
      "Map: 100%|██████████| 559/559 [00:00<00:00, 998.74 examples/s] \n"
     ]
    }
   ],
   "source": [
    "#Now, let's apply the preprocessing function to the entire dataset:\n",
    "col_to_delete = ['search_phrase', 'mistral_pred','openhermes_pred', 'vicuna_pred', 'gemma_pred']\n",
    "#Apply the preprocessing function\n",
    "roberta_tokenized_datasets = data.map(roberta_preprocessing_function, batched=False)\n",
    "#Remove the undesired columns\n",
    "roberta_tokenized_datasets = roberta_tokenized_datasets.remove_columns(col_to_delete)\n",
    "#Rename the target to label as for HugginFace standards\n",
    "roberta_tokenized_datasets = roberta_tokenized_datasets.rename_column(\"email\", \"text\")\n",
    "#Set to torch format\n",
    "roberta_tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(0),\n",
       " 'text': 'When shares of Enron plunged from $84 earlier this year to practically zero=, thousands of the company\\'s employees lost not just their jobs but also mo=st of the value of their 401(k) retirement accounts. For the average employ=ee, Enron stock represented three-fifths of 401(k) assets, and the energy c=ompany\\'s meltdown -- after revelations of misleading, probably fraudulent, =accounting practices -- was a personal calamity.=20Now politicians, including Democratic Sens. Barbara Boxer of California and= Jon Corzine of New Jersey, are rushing in to protect other Americans from =similar disasters. And President Bush has ordered \"a policy review to prote=ct people\\'s pensions.\" But government intervention would only introduce a d=angerous idea: that investors shouldn\\'t bear the burden of their own decisi=ons.Let\\'s be clear: Enron executives and outside auditors who lied to investors= -- including their own 21,000 employees -- bear an enormous responsibility=, moral and legal. But the employees also bear some responsibility. Most of= them had far too large a proportion of their retirement assets tied up in =their own company. They took a risk.=20Enron offered a typical 401(k): Employees could invest up to 6% of their ba=se pay in a wide range of options, including stock mutual funds like Fideli=ty Magellan, bonds, money-market funds and a self-directed Schwab account t=hat could buy practically anything. They could also choose Enron stock, pur=chased at the regular market price. Whatever employees contributed with the=ir own money, the company matched, up to 50% (that is, 3% of base pay), wit=h Enron stock.=20In other words, free Enron stock -- $1,800 worth a year for an employee mak=ing $60,000 in base pay -- was part of the compensation package. Workers kn=ew it, and they presumably liked it. Workers also knew that their 401(k) ha=d a rule, also common to such plans, that they had to keep the company stoc=k given to them by Enron until they were at least 50 years old. Any Enron s=tock they bought themselves, of course, they could transfer at will.=20Enron stock soared in value -- but turned out to be a blessing and a curse.= Imagine the case of an employee who joined the company in 1997, when the s=tock was worth (after adjusting for splits) about $20 a share. If the emplo=yee bought other assets which grew at 10% a year, by the end of 2000 those =assets had grown by about one-third while the Enron stock had more than qua=drupled. His 401(k) account became lopsided, with far more Enron stock than= anything else.=20That was a predicament common to many Enron employees when their company st=ock crashed. The wise move, as Enron climbed, would have been to buy other =assets for a separate, taxable plan to balance the company stock. How many =employees did that? Probably not many -- presumably for some because they d=idn\\'t have the money. If Social Security had been reformed, they could have=',\n",
       " 'input_ids': tensor([    0,   520,   327,     9,  2271,  2839, 12662,    31,    68,  6232,\n",
       "           656,    42,    76,     7, 15655,  4276,  5214,     6,  1583,     9,\n",
       "             5,   138,    18,  1321,   685,    45,    95,    49,  1315,    53,\n",
       "            67,  7458,  5214,   620,     9,     5,   923,     9,    49, 17936,\n",
       "          1640,   330,    43,  3832,  2349,     4,   286,     5,   674, 12735,\n",
       "          5214,  1942,     6,  2271,  2839,   388,  4625,   130,    12, 41830,\n",
       "         15561,     9, 17936,  1640,   330,    43,  1781,     6,     8,     5,\n",
       "          1007,   740,  5214,  7474,  3785,    18, 24053,   480,    71, 14613,\n",
       "             9, 12030,     6,  1153, 15381,     6,  5457, 36617,   154,  3464,\n",
       "           480,    21,    10,  1081, 28857,  1571,     4,  5214,   844,  5975,\n",
       "          3770,     6,   217,  1557, 12274,     4,  4810,  7803,   254,     9,\n",
       "           886,     8,  5214,  4160,  2812,   329,   833,     9,   188,  3123,\n",
       "             6,    32,  6404,    11,     7,  1744,    97,  1791,    31,  5457,\n",
       "         42116, 12884,     4,   178,   270,  3516,    34,  2740,    22,   102,\n",
       "           714,  1551,     7, 27067,  5214,  3894,    82,    18, 15131,    72,\n",
       "           125,   168,  6530,    74,   129,  6581,    10,   385,  5214,  8395,\n",
       "          1827,  1114,    35,    14,   867,  4395,    75,  4649,     5,  6976,\n",
       "             9,    49,   308,  5044,  8539,  5214,  1790,     4,  7939,    18,\n",
       "            28,   699,    35,  2271,  2839,  4585,     8,   751,  9818,  9314,\n",
       "            54, 15005,     7,   867,  5214,   480,   217,    49,   308,   733,\n",
       "             6,   151,  1321,   480,  4649,    41,  7934,  2640,  5214,     6,\n",
       "          7654,     8,  1030,     4,   125,     5,  1321,    67,  4649,   103,\n",
       "          2640,     4,  1993,     9,  5214,   106,    56,   444,   350,   739,\n",
       "            10, 10301,     9,    49,  3832,  1781,  3016,    62,    11,  5457,\n",
       "         25017,   308,   138,     4,   252,   362,    10,   810,     4,  5214,\n",
       "           844, 16040,  2839,  1661,    10,  6097, 17936,  1640,   330,  3256,\n",
       "         13479,   115,  3754,    62,     7,   231,   207,     9,    49, 17279,\n",
       "          5214,  1090,   582,    11,    10,  1810,  1186,     9,  1735,     6,\n",
       "           217,   388,  7628,  1188,   101, 30424,   118,  5214,  2553, 28387,\n",
       "         30351,     6,  3554,     6,   418,    12,  2989,  1188,     8,    10,\n",
       "          1403,    12, 25706, 14376,   873,  1316,   326,  5214,  8849,   115,\n",
       "           907, 15655,   932,     4,   252,   115,    67,  2807,  2271,  2839,\n",
       "           388,     6,  9695,  5214,   611, 11835,    23,     5,  1675,   210,\n",
       "           425,     4, 11999,  1321,  3162,    19,     5,  5214,   853,   308,\n",
       "           418,     6,     5,   138,  9184,     6,    62,     7,   654,   207,\n",
       "            36,  6025,    16,     6,   155,   207,     9,  1542,   582,   238,\n",
       "         22094,  5214,   298,  2271,  2839,   388,     4,  5214,   844,  1121,\n",
       "            97,  1617,     6,   481,  2271,  2839,   388,   480,    68,   134,\n",
       "             6,  3913,   966,    10,    76,    13,    41,  3200,   475,   677,\n",
       "          5214,   154,    68,  2466,     6,   151,    11,  1542,   582,   480,\n",
       "            21,   233,     9,     5,  4660,  3737,     4, 10586, 11269,  5214,\n",
       "          2753,    24,     6,     8,    51, 15546,  6640,    24,     4, 10586,\n",
       "            67,  1467,    14,    49, 17936,  1640,   330,    43,  2489,  5214,\n",
       "           417,    10,  2178,     6,    67,  1537,     7,   215,   708,     6,\n",
       "            14,    51,    56,     7,   489,     5,   138,  1690,  1975,  5214,\n",
       "           330,   576,     7,   106,    30,  2271,  2839,   454,    51,    58,\n",
       "            23,   513,   654,   107,   793,     4,  5053,  2271,  2839,   579,\n",
       "          5214,    90,  3343,    51,  2162,  1235,     6,     9,   768,     6,\n",
       "            51,   115,  2937,    23,    40,     4,  5214,   844, 16040,  2839,\n",
       "           388, 14622,    11,   923,   480,    53,  1224,    66,     7,    28,\n",
       "            10, 14164,     8,    10, 26020,     4,  5214, 15467,     5,   403,\n",
       "             9,     2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can have a look into our tokenized training dataset:\n",
    "roberta_tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "from transformers import DataCollatorWithPadding\n",
    "roberta_data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa\n",
    "\n",
    "A. Load checkpoints for the classfication model\n",
    "\n",
    "We load the pre-trained RoBERTa model with a sequence classification header using the HuggingFace AutoModelForSequenceClassification class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification \n",
    "# Load a pre-trained model with a sequence classification header \n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. LoRa setup for RoBERTa classifier\n",
    "\n",
    "We import LoRa configuration and set some parameters for RoBERTa classifier:\n",
    "\n",
    "TaskType: Sequence classification\n",
    "r(rank): Rank for our decomposition matrices\n",
    "\n",
    "lora_alpha: Alpha parameter to scale the learned weights. LoRA paper advises fixing alpha at 16\n",
    "\n",
    "lora_dropout: Dropout probability of the LoRA layers\n",
    "\n",
    "bias: Whether to add bias term to LoRa layers\n",
    "\n",
    "In the code below, we use the values recommended by the Lora paper.\n",
    "\n",
    "Note that in this blog, we will perform a hyperparameters tunning with wandb.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,299,908 || all params: 356,610,052 || trainable%: 0.6449363911929212\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "roberta_peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=2, lora_alpha=16, lora_dropout=0.1, bias=\"none\",\n",
    ")\n",
    "roberta_model = get_peft_model(roberta_model, roberta_peft_config)\n",
    "roberta_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the trainer\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "First, we define the perfomance metrics we will use to compare the three models: F1 score, recall, precision and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#Define evaluation metrics\n",
    "import evaluate\n",
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    # All metrics are already predefined in the HF `evaluate` package\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric= evaluate.load(\"f1\")\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    # The trainer is expecting a dictionary where the keys are the metrics names and the values are the scores. \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Trainer for Weighted Loss\n",
    "\n",
    "As mentioned at the begining of this post, we have an imbalanced distribution between positive and negative classes. To account for that, we need to train our models with a weight cross-entropy loss. The Trainer class doesn't support providing a custom loss as it expects getting the loss directly from the model's outputs.\n",
    "\n",
    "So, we need to define our custom WeightedCELossTrainer that overrides the compute_loss method to calculate the weighted cross-entropy loss based on the model's predictions and the input labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "class WeightedCELossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # Get model's predictions\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([neg_weights, pos_weights], device=model.device, dtype=logits.dtype))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Setup\n",
    "\n",
    "Let's set the training arguments and the trainer for the three models.\n",
    "\n",
    "### A. RoBERTa\n",
    "\n",
    "First important step is to move the models to the GPU device for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m roberta_model \u001b[38;5;241m=\u001b[39m \u001b[43mroberta_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m roberta_model\u001b[38;5;241m.\u001b[39mdevice()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#It will print the following:\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/External/opt/anaconda3/envs/ollama2/lib/python3.11/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/External/opt/anaconda3/envs/ollama2/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/External/opt/anaconda3/envs/ollama2/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/Volumes/External/opt/anaconda3/envs/ollama2/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/External/opt/anaconda3/envs/ollama2/lib/python3.11/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/Volumes/External/opt/anaconda3/envs/ollama2/lib/python3.11/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Volumes/External/opt/anaconda3/envs/ollama2/lib/python3.11/site-packages/torch/cuda/__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "roberta_model = roberta_model.cuda()\n",
    "roberta_model.device()\n",
    "\n",
    "#It will print the following:\n",
    "device(type='cuda', index=0)\n",
    "\n",
    "#Define the training arguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "lr = 1e-4\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"roberta-large-lora-token-classification\",\n",
    "    learning_rate=lr,\n",
    "    lr_scheduler_type= \"constant\",\n",
    "    warmup_ratio= 0.1,\n",
    "    max_grad_norm= 0.3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.001,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "#Finally, we define the RoBERTa trainer by providing the model, the training arguments and the tokenized datasets:\n",
    "\n",
    "roberta_trainer = WeightedCELossTrainer(\n",
    "    model=roberta_model,\n",
    "    args=training_args,\n",
    "    train_dataset=roberta_tokenized_datasets['train'],\n",
    "    eval_dataset=roberta_tokenized_datasets[\"val\"],\n",
    "    data_collator=roberta_data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
