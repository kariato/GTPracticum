{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "  Downloading mxnet-1.6.0-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting numpy<2.0.0,>1.16.0 (from mxnet)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /Volumes/External/minic/envs/dsv1/lib/python3.11/site-packages (from mxnet) (2.32.3)\n",
      "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Volumes/External/minic/envs/dsv1/lib/python3.11/site-packages (from requests<3,>=2.20.0->mxnet) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Volumes/External/minic/envs/dsv1/lib/python3.11/site-packages (from requests<3,>=2.20.0->mxnet) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Volumes/External/minic/envs/dsv1/lib/python3.11/site-packages (from requests<3,>=2.20.0->mxnet) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Volumes/External/minic/envs/dsv1/lib/python3.11/site-packages (from requests<3,>=2.20.0->mxnet) (2025.1.31)\n",
      "Downloading mxnet-1.6.0-py2.py3-none-any.whl (68.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Installing collected packages: numpy, graphviz, mxnet\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "Successfully installed graphviz-0.8.4 mxnet-1.6.0 numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet #autogluon.tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import random\n",
    "from autogluon.tabular import TabularPredictor\n",
    "#import mxnet as mx\n",
    "\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "#mx.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-01 12:55:06--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv\n",
      "Resolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.213.57, 52.216.184.179, 3.5.29.97, ...\n",
      "Connecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.213.57|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 689486 (673K) [text/csv]\n",
      "Saving to: ‘product_sentiment_machine_hack/train.csv’\n",
      "\n",
      "product_sentiment_m 100%[===================>] 673.33K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2025-04-01 12:55:06 (7.77 MB/s) - ‘product_sentiment_machine_hack/train.csv’ saved [689486/689486]\n",
      "\n",
      "--2025-04-01 12:55:07--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv\n",
      "Resolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.1.228, 52.216.184.179, 3.5.29.97, ...\n",
      "Connecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.1.228|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75517 (74K) [text/csv]\n",
      "Saving to: ‘product_sentiment_machine_hack/dev.csv’\n",
      "\n",
      "product_sentiment_m 100%[===================>]  73.75K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-04-01 12:55:07 (2.53 MB/s) - ‘product_sentiment_machine_hack/dev.csv’ saved [75517/75517]\n",
      "\n",
      "--2025-04-01 12:55:07--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv\n",
      "Resolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 16.182.71.17, 52.216.184.179, 52.217.116.9, ...\n",
      "Connecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|16.182.71.17|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 312194 (305K) [text/csv]\n",
      "Saving to: ‘product_sentiment_machine_hack/test.csv’\n",
      "\n",
      "product_sentiment_m 100%[===================>] 304.88K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2025-04-01 12:55:08 (4.85 MB/s) - ‘product_sentiment_machine_hack/test.csv’ saved [312194/312194]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p product_sentiment_machine_hack\n",
    "!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv -O product_sentiment_machine_hack/train.csv\n",
    "!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv -O product_sentiment_machine_hack/dev.csv\n",
    "!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv -O product_sentiment_machine_hack/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 2000\n",
      "Number of dev samples: 637\n",
      "Number of test samples: 2728\n"
     ]
    }
   ],
   "source": [
    "subsample_size = 2000  # for quick demo, try setting to larger values\n",
    "feature_columns = ['Product_Description', 'Product_Type']\n",
    "label = 'Sentiment'\n",
    "\n",
    "train_df = pd.read_csv('product_sentiment_machine_hack/train.csv', index_col=0).sample(2000, random_state=123)\n",
    "dev_df = pd.read_csv('product_sentiment_machine_hack/dev.csv', index_col=0)\n",
    "test_df = pd.read_csv('product_sentiment_machine_hack/test.csv', index_col=0)\n",
    "\n",
    "train_df = train_df[feature_columns + [label]]\n",
    "dev_df = dev_df[feature_columns + [label]]\n",
    "test_df = test_df[feature_columns]\n",
    "print('Number of training samples:', len(train_df))\n",
    "print('Number of dev samples:', len(dev_df))\n",
    "print('Number of test samples:', len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Description</th>\n",
       "      <th>Product_Type</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4532</th>\n",
       "      <td>they took away the lego pit but replaced it wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>#Apple to Open Pop-Up Shop at #SXSW [REPORT]: ...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3536</th>\n",
       "      <td>RT @mention False Alarm: Google Circles Not Co...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5157</th>\n",
       "      <td>Will Google reveal a new social network called...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4643</th>\n",
       "      <td>Niceness RT @mention Less than 2 hours until w...</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Product_Description  Product_Type  \\\n",
       "4532  they took away the lego pit but replaced it wi...             0   \n",
       "1831  #Apple to Open Pop-Up Shop at #SXSW [REPORT]: ...             9   \n",
       "3536  RT @mention False Alarm: Google Circles Not Co...             5   \n",
       "5157  Will Google reveal a new social network called...             9   \n",
       "4643  Niceness RT @mention Less than 2 hours until w...             6   \n",
       "\n",
       "      Sentiment  \n",
       "4532          1  \n",
       "1831          2  \n",
       "3536          1  \n",
       "5157          2  \n",
       "4643          3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_tabular_product_sentiment_multimodal\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 24.3.0: Thu Jan  2 20:23:36 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       3.68 GB / 16.00 GB (23.0%)\n",
      "Disk Space Avail:   56.93 GB / 1862.82 GB (3.1%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/Volumes/External/source/GTPracticum/EDA/ag_tabular_product_sentiment_multimodal\"\n",
      "Train Data Rows:    2000\n",
      "Train Data Columns: 2\n",
      "Label Column:       Sentiment\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\t4 unique label values:  [np.int64(1), np.int64(2), np.int64(3), np.int64(0)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 4\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3767.29 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Product_Description']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 230\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])          : 1 | ['Product_Type']\n",
      "\t\t('object', ['text']) : 1 | ['Product_Description']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', ['text_as_category'])  :   1 | ['Product_Description']\n",
      "\t\t('int', [])                         :   1 | ['Product_Type']\n",
      "\t\t('int', ['binned', 'text_special']) :  30 | ['Product_Description.char_count', 'Product_Description.word_count', 'Product_Description.capital_ratio', 'Product_Description.lower_ratio', 'Product_Description.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 224 | ['__nlp__.about', '__nlp__.all', '__nlp__.amp', '__nlp__.an', '__nlp__.an ipad', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t2 features in original data used to generate 256 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.93 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.84s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1600, Val Rows: 400\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.68\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.6875\t = Validation score   (accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.2`. \n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.2`.\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.2`.\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.8875\t = Validation score   (accuracy)\n",
      "\t2.01s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.8825\t = Validation score   (accuracy)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tWarning: Exception caused CatBoost to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.2`.\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.8575\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.8575\t = Validation score   (accuracy)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tWarning: Exception caused XGBoost to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.2`.\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training (ImportError)... Skipping this model.\n",
      "\t\tUnable to import dependency torch\n",
      "A quick tip is to install via `pip install torch`.\n",
      "The minimum torch version is currently 2.2.\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.2`.\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'RandomForestGini': 1.0}\n",
      "\t0.8875\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4.69s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 15021.9 rows/s (400 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Volumes/External/source/GTPracticum/EDA/ag_tabular_product_sentiment_multimodal\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x1770d2990>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "predictor = TabularPredictor(label='Sentiment', path='ag_tabular_product_sentiment_multimodal')\n",
    "predictor.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestEntr</td>\n",
       "      <td>0.883830</td>\n",
       "      <td>0.8825</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.057359</td>\n",
       "      <td>0.026910</td>\n",
       "      <td>0.263062</td>\n",
       "      <td>0.057359</td>\n",
       "      <td>0.026910</td>\n",
       "      <td>0.263062</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestGini</td>\n",
       "      <td>0.883830</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.098588</td>\n",
       "      <td>0.026416</td>\n",
       "      <td>2.011341</td>\n",
       "      <td>0.098588</td>\n",
       "      <td>0.026416</td>\n",
       "      <td>2.011341</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.883830</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.101279</td>\n",
       "      <td>0.026628</td>\n",
       "      <td>2.021191</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.009850</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTreesGini</td>\n",
       "      <td>0.871272</td>\n",
       "      <td>0.8575</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.044516</td>\n",
       "      <td>0.026974</td>\n",
       "      <td>0.246803</td>\n",
       "      <td>0.044516</td>\n",
       "      <td>0.026974</td>\n",
       "      <td>0.246803</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ExtraTreesEntr</td>\n",
       "      <td>0.864992</td>\n",
       "      <td>0.8575</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.049226</td>\n",
       "      <td>0.026893</td>\n",
       "      <td>0.260282</td>\n",
       "      <td>0.049226</td>\n",
       "      <td>0.026893</td>\n",
       "      <td>0.260282</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNeighborsDist</td>\n",
       "      <td>0.723705</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.047603</td>\n",
       "      <td>0.044602</td>\n",
       "      <td>0.019329</td>\n",
       "      <td>0.047603</td>\n",
       "      <td>0.044602</td>\n",
       "      <td>0.019329</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNeighborsUnif</td>\n",
       "      <td>0.720565</td>\n",
       "      <td>0.6800</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.054610</td>\n",
       "      <td>0.106696</td>\n",
       "      <td>0.008695</td>\n",
       "      <td>0.054610</td>\n",
       "      <td>0.106696</td>\n",
       "      <td>0.008695</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_test  score_val eval_metric  pred_time_test  \\\n",
       "0     RandomForestEntr    0.883830     0.8825    accuracy        0.057359   \n",
       "1     RandomForestGini    0.883830     0.8875    accuracy        0.098588   \n",
       "2  WeightedEnsemble_L2    0.883830     0.8875    accuracy        0.101279   \n",
       "3       ExtraTreesGini    0.871272     0.8575    accuracy        0.044516   \n",
       "4       ExtraTreesEntr    0.864992     0.8575    accuracy        0.049226   \n",
       "5       KNeighborsDist    0.723705     0.6875    accuracy        0.047603   \n",
       "6       KNeighborsUnif    0.720565     0.6800    accuracy        0.054610   \n",
       "\n",
       "   pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
       "0       0.026910  0.263062                 0.057359                0.026910   \n",
       "1       0.026416  2.011341                 0.098588                0.026416   \n",
       "2       0.026628  2.021191                 0.002691                0.000212   \n",
       "3       0.026974  0.246803                 0.044516                0.026974   \n",
       "4       0.026893  0.260282                 0.049226                0.026893   \n",
       "5       0.044602  0.019329                 0.047603                0.044602   \n",
       "6       0.106696  0.008695                 0.054610                0.106696   \n",
       "\n",
       "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
       "0           0.263062            1       True          4  \n",
       "1           2.011341            1       True          3  \n",
       "2           0.009850            2       True          7  \n",
       "3           0.246803            1       True          5  \n",
       "4           0.260282            1       True          6  \n",
       "5           0.019329            1       True          2  \n",
       "6           0.008695            1       True          1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown `.fit` keyword argument specified: 'presets'\nValid kwargs: ['ag_args', 'ag_args_ensemble', 'ag_args_fit', 'calibrate', 'delay_bag_sets', 'excluded_model_types', 'feature_prune_kwargs', 'hyperparameter_tune_kwargs', 'included_model_types', 'keep_only_best', 'name_suffix', 'num_bag_sets', 'num_stack_levels', 'pseudo_data', 'raise_on_no_models_fitted', 'refit_full', 'save_bag_folds', 'save_space', 'set_best_to_refit_full', 'verbosity']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictor\u001b[38;5;241m.\u001b[39mfit_extra(train_df, presets\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_quality\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Volumes/External/minic/envs/dsv1/lib/python3.11/site-packages/autogluon/tabular/predictor/predictor.py:1856\u001b[0m, in \u001b[0;36mTabularPredictor.fit_extra\u001b[0;34m(self, hyperparameters, time_limit, base_model_names, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, num_cpus, num_gpus, fit_strategy, memory_limit, **kwargs)\u001b[0m\n\u001b[1;32m   1853\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   1855\u001b[0m kwargs_orig \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m-> 1856\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_extra_kwargs(kwargs)\n\u001b[1;32m   1858\u001b[0m verbosity \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity)\n\u001b[1;32m   1859\u001b[0m set_logger_verbosity(verbosity)\n",
      "File \u001b[0;32m/Volumes/External/minic/envs/dsv1/lib/python3.11/site-packages/autogluon/tabular/predictor/predictor.py:5100\u001b[0m, in \u001b[0;36mTabularPredictor._validate_fit_extra_kwargs\u001b[0;34m(self, kwargs, extra_valid_keys)\u001b[0m\n\u001b[1;32m   5098\u001b[0m         public_kwarg_options \u001b[38;5;241m=\u001b[39m [kwarg \u001b[38;5;28;01mfor\u001b[39;00m kwarg \u001b[38;5;129;01min\u001b[39;00m allowed_kwarg_names \u001b[38;5;28;01mif\u001b[39;00m kwarg[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   5099\u001b[0m         public_kwarg_options\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m-> 5100\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown `.fit` keyword argument specified: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwarg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mValid kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpublic_kwarg_options\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5102\u001b[0m kwargs_sanitized \u001b[38;5;241m=\u001b[39m fit_extra_kwargs_default\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   5103\u001b[0m kwargs_sanitized\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown `.fit` keyword argument specified: 'presets'\nValid kwargs: ['ag_args', 'ag_args_ensemble', 'ag_args_fit', 'calibrate', 'delay_bag_sets', 'excluded_model_types', 'feature_prune_kwargs', 'hyperparameter_tune_kwargs', 'included_model_types', 'keep_only_best', 'name_suffix', 'num_bag_sets', 'num_stack_levels', 'pseudo_data', 'raise_on_no_models_fitted', 'refit_full', 'save_bag_folds', 'save_space', 'set_best_to_refit_full', 'verbosity']"
     ]
    }
   ],
   "source": [
    "predictor.fit_extra(train_df, presets='best_quality')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
