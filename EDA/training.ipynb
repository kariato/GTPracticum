{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Training script to fine-tune a pre-train LLM with PEFT methods using HuggingFace.\n",
    "  Example to run this conversion script:\n",
    "    python peft_training.py \\\n",
    "     --in-file <path_to_hf_checkpoints_folder> \\\n",
    "     --out-file <path_to_output_nemo_file> \\\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"] = \"peft_tweets\" # log to your project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"all\" # log your models\n",
    "from copy import deepcopy\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from datasets import load_from_disk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
    "import torch\n",
    "\n",
    "POS_WEIGHT, NEG_WEIGHT = (1.1637114032405993, 0.8766697374481806)\n",
    "\n",
    "def get_args():\n",
    "    parser = ArgumentParser(description=\"Fine-tune an LLM model with PEFT\")\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"Path to Huggingface pre-processed dataset\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"Path to store the fine-tuned model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"Name of the pre-trained LLM to fine-tune\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_length\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        required=False,\n",
    "        help=\"Maximum length of the input sequences\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--set_pad_id\", \n",
    "        action=\"store_true\",\n",
    "        help=\"Set the id for the padding token, needed by models such as Mistral-7B\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\", type=float, default=1e-3, help=\"Learning rate for training\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\", type=int, default=64, help=\"Train batch size\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_batch_size\", type=int, default=64, help=\"Eval batch size\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\", type=int, default=5, help=\"Number of epochs\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--weight_decay\", type=float, default=0.1, help=\"Weight decay\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lora_rank\", type=int, default=4, help=\"Lora rank\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lora_alpha\", type=float, default=0.0, help=\"Lora alpha\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lora_dropout\", type=float, default=0.2, help=\"Lora dropout\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lora_bias\",\n",
    "        type=str,\n",
    "        default='none',\n",
    "        choices={\"lora_only\", \"none\", 'all'},\n",
    "        help=\"Layers to add learnable bias\"\n",
    "    )\n",
    "\n",
    "    arguments = parser.parse_args()\n",
    "    return arguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric= evaluate.load(\"f1\")\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}\n",
    "\n",
    "\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def __init__(self, trainer) -> None:\n",
    "        super().__init__()\n",
    "        self._trainer = trainer\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = deepcopy(control)\n",
    "            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            return control_copy\n",
    "\n",
    "\n",
    "def get_dataset_and_collator(\n",
    "    data_path,\n",
    "    model_checkpoints,\n",
    "    add_prefix_space=True,\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    set_pad_id=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the preprocessed HF dataset with train, valid and test objects\n",
    "    \n",
    "    Paramters:\n",
    "    ---------\n",
    "    data_path: str \n",
    "        Path to the pre-processed HuggingFace dataset \n",
    "    model_checkpoints: \n",
    "        Name of the pre-trained model to use for tokenization\n",
    "    \"\"\"\n",
    "    data = load_from_disk(data_path)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_checkpoints,\n",
    "        add_prefix_space=add_prefix_space\n",
    "    )\n",
    "\n",
    "    if set_pad_id:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def _preprocesscing_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=truncation, max_length=max_length)\n",
    "\n",
    "    col_to_delete = ['id', 'keyword','location', 'text']\n",
    "    tokenized_datasets = data.map(_preprocesscing_function, batched=False)\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns(col_to_delete)\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "    padding_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    return tokenized_datasets, padding_collator\n",
    "\n",
    "\n",
    "def get_lora_model(model_checkpoints, num_labels=2, rank=4, alpha=16, lora_dropout=0.1, bias='none'):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    #if model_checkpoints == 'mistralai/Mistral-7B-v0.1' : \n",
    "    model =  AutoModelForSequenceClassification.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_checkpoints,\n",
    "            num_labels=num_labels,\n",
    "            device_map=\"auto\",\n",
    "            offload_folder=\"offload\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    if model_checkpoints == 'mistralai/Mistral-7B-v0.1' or model_checkpoints == 'meta-llama/Llama-2-7b-hf': \n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS, r=rank, lora_alpha=alpha, lora_dropout=lora_dropout, bias=bias, \n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"v_proj\",\n",
    "            ],\n",
    "    )\n",
    "    else: \n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS, r=rank, lora_alpha=alpha, lora_dropout=lora_dropout, bias=bias,\n",
    "        )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    print(model.print_trainable_parameters())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_weighted_trainer(pos_weight, neg_weight):\n",
    "    \n",
    "    class _WeightedBCELossTrainer(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            labels = inputs.pop(\"labels\")\n",
    "            # forward pass\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.get(\"logits\")\n",
    "            # compute custom loss (suppose one has 3 labels with different weights)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([neg_weight, pos_weight], device=labels.device, dtype=logits.dtype))\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "    return _WeightedBCELossTrainer\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"\n",
    "    Training function\n",
    "    \"\"\"\n",
    "\n",
    "    dataset, collator =  get_dataset_and_collator(\n",
    "        args.data_path,\n",
    "        args.model_name,\n",
    "        max_length=args.max_length,\n",
    "        set_pad_id=args.set_pad_id,\n",
    "        add_prefix_space=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_path,\n",
    "        learning_rate=args.lr,\n",
    "        lr_scheduler_type= \"cosine\",\n",
    "        warmup_ratio= 0.1,\n",
    "        per_device_train_batch_size=args.train_batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        num_train_epochs=args.num_epochs,\n",
    "        weight_decay=args.weight_decay,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,\n",
    "        report_to=\"wandb\",\n",
    "        max_grad_norm= 0.3,\n",
    "    )\n",
    "\n",
    "    model = get_lora_model(\n",
    "        args.model_name,\n",
    "        rank=args.lora_rank,\n",
    "        alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=args.lora_bias\n",
    "    )\n",
    "    if args.set_pad_id: \n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    # move model to GPU device\n",
    "    if model.device.type != 'cuda':\n",
    "        model=model.to('cuda')\n",
    "\n",
    "    \n",
    "    weighted_trainer = get_weighted_trainer(POS_WEIGHT, NEG_WEIGHT)\n",
    "    \n",
    "    trainer = weighted_trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset[\"val\"],\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.add_callback(CustomCallback(trainer))\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
