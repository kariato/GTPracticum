{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this Jupyter Notebook is sourced from the following web page: [GitHub - Lora for sequence classification with Roberta-Llama-Mistral](https://github.com/mehdiir/Roberta-Llama-Mistral/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512 \n",
    "roberta_checkpoint = \"roberta-large\"\n",
    "mistral_checkpoint = \"mistralai/Mistral-7B-v0.1\"\n",
    "llama_checkpoint = \"meta-llama/Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation\n",
    "\n",
    "### Read in the training data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2236 entries, 0 to 2235\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   search_phrase    2236 non-null   object \n",
      " 1   label            2236 non-null   int64  \n",
      " 2   email            2236 non-null   object \n",
      " 3   mistral_pred     2236 non-null   float64\n",
      " 4   openhermes_pred  2236 non-null   float64\n",
      " 5   vicuna_pred      2236 non-null   float64\n",
      " 6   gemma_pred       2236 non-null   float64\n",
      "dtypes: float64(4), int64(1), object(2)\n",
      "memory usage: 122.4+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 559 entries, 0 to 558\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   search_phrase    559 non-null    object \n",
      " 1   label            559 non-null    int64  \n",
      " 2   email            559 non-null    object \n",
      " 3   mistral_pred     559 non-null    float64\n",
      " 4   openhermes_pred  559 non-null    float64\n",
      " 5   vicuna_pred      559 non-null    float64\n",
      " 6   gemma_pred       559 non-null    float64\n",
      " 7   target           559 non-null    int64  \n",
      "dtypes: float64(4), int64(2), object(2)\n",
      "memory usage: 35.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "DATA_PATH = \"../data/\"\n",
    "train_df=pd.read_csv(os.path.join(DATA_PATH, 'enron_labeled_curated_train.csv'))\n",
    "test_df=pd.read_csv(os.path.join(DATA_PATH, 'enron_labeled_curated_test.csv'))\n",
    "# dummy target column for merge test and train into one huggingface data\n",
    "test_df['target'] = 0 \n",
    "train_df.info()\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the classes are not balanced, we will compute the positive and negative weights and use them for loss calculation later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    1918\n",
      "1     318\n",
      "Name: count, dtype: int64\n",
      "3.5157232704402515 0.5828988529718456\n"
     ]
    }
   ],
   "source": [
    "print(train_df.label.value_counts())\n",
    "pos_weights = len(train_df) / (2 * train_df.label.value_counts()[1])\n",
    "neg_weights = len(train_df) / (2 * train_df.label.value_counts()[0])\n",
    "POS_WEIGHT, NEG_WEIGHT = (pos_weights, neg_weights)\n",
    "print(POS_WEIGHT, NEG_WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of character is 2983.\n",
      "The maximum number of word is 650.\n"
     ]
    }
   ],
   "source": [
    "##Then, we compute the maximum length of the column text:\n",
    "# Number of Characters\n",
    "max_char=train_df['email'].str.len().max()\n",
    "# Number of Words\n",
    "max_words = train_df['email'].str.split().str.len().max()\n",
    "print(f\"The maximum number of character is {max_char}.\")\n",
    "print(f\"The maximum number of word is {max_words}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/External/opt/anaconda3/envs/ollama2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1788/1788 [00:00<00:00, 72644.12 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 448/448 [00:00<00:00, 38764.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 559/559 [00:00<00:00, 155447.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#Now, let's convert the dataframe into HuggingFace dataset format, \n",
    "#split it into training and validation datasets, add the test dataset and save the three files:\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "#Convert the training dataframe to HuggingFace dataset\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "#Split the dataset into training and validation datasets\n",
    "data = dataset.train_test_split(train_size=0.8, seed=42)\n",
    "#Rename the default \"test\" split to \"validation\"\n",
    "data['val'] = data.pop(\"test\")\n",
    "#Convert the test dataframe to HuggingFace dataset and add it into the first dataset\n",
    "data['test'] = Dataset.from_pandas(test_df)\n",
    "#Save\n",
    "data.save_to_disk(\"processed_hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'search_phrase': 'it appears that some Enron employees used dummy accounts and rigged valuation methodologies to create false profit-and-loss entries for the derivatives',\n",
       " 'label': 0,\n",
       " 'email': 'When shares of Enron plunged from $84 earlier this year to practically zero=, thousands of the company\\'s employees lost not just their jobs but also mo=st of the value of their 401(k) retirement accounts. For the average employ=ee, Enron stock represented three-fifths of 401(k) assets, and the energy c=ompany\\'s meltdown -- after revelations of misleading, probably fraudulent, =accounting practices -- was a personal calamity.=20Now politicians, including Democratic Sens. Barbara Boxer of California and= Jon Corzine of New Jersey, are rushing in to protect other Americans from =similar disasters. And President Bush has ordered \"a policy review to prote=ct people\\'s pensions.\" But government intervention would only introduce a d=angerous idea: that investors shouldn\\'t bear the burden of their own decisi=ons.Let\\'s be clear: Enron executives and outside auditors who lied to investors= -- including their own 21,000 employees -- bear an enormous responsibility=, moral and legal. But the employees also bear some responsibility. Most of= them had far too large a proportion of their retirement assets tied up in =their own company. They took a risk.=20Enron offered a typical 401(k): Employees could invest up to 6% of their ba=se pay in a wide range of options, including stock mutual funds like Fideli=ty Magellan, bonds, money-market funds and a self-directed Schwab account t=hat could buy practically anything. They could also choose Enron stock, pur=chased at the regular market price. Whatever employees contributed with the=ir own money, the company matched, up to 50% (that is, 3% of base pay), wit=h Enron stock.=20In other words, free Enron stock -- $1,800 worth a year for an employee mak=ing $60,000 in base pay -- was part of the compensation package. Workers kn=ew it, and they presumably liked it. Workers also knew that their 401(k) ha=d a rule, also common to such plans, that they had to keep the company stoc=k given to them by Enron until they were at least 50 years old. Any Enron s=tock they bought themselves, of course, they could transfer at will.=20Enron stock soared in value -- but turned out to be a blessing and a curse.= Imagine the case of an employee who joined the company in 1997, when the s=tock was worth (after adjusting for splits) about $20 a share. If the emplo=yee bought other assets which grew at 10% a year, by the end of 2000 those =assets had grown by about one-third while the Enron stock had more than qua=drupled. His 401(k) account became lopsided, with far more Enron stock than= anything else.=20That was a predicament common to many Enron employees when their company st=ock crashed. The wise move, as Enron climbed, would have been to buy other =assets for a separate, taxable plan to balance the company stock. How many =employees did that? Probably not many -- presumably for some because they d=idn\\'t have the money. If Social Security had been reformed, they could have=',\n",
       " 'mistral_pred': 0.0,\n",
       " 'openhermes_pred': 0.0,\n",
       " 'vicuna_pred': 0.7,\n",
       " 'gemma_pred': 0.6}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1788/1788 [00:01<00:00, 1130.62 examples/s]\n",
      "Map: 100%|██████████| 448/448 [00:00<00:00, 1264.02 examples/s]\n",
      "Map: 100%|██████████| 559/559 [00:00<00:00, 1341.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Mistral 7B Tokenizer\n",
    "# Add prefix space to tokenize words into subwords\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_checkpoint,  add_prefix_space=True)\n",
    "mistral_tokenizer.pad_token_id = mistral_tokenizer.eos_token_id\n",
    "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "\n",
    "def mistral_preprocessing_function(examples):\n",
    "    return mistral_tokenizer(examples['email'], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "#Now, let's apply the preprocessing function to the entire dataset:\n",
    "col_to_delete = ['search_phrase', 'mistral_pred','openhermes_pred', 'vicuna_pred', 'gemma_pred']\n",
    "mistral_tokenized_datasets = data.map(mistral_preprocessing_function, batched=False)\n",
    "mistral_tokenized_datasets = mistral_tokenized_datasets.remove_columns(col_to_delete)\n",
    "mistral_tokenized_datasets = mistral_tokenized_datasets.rename_column(\"email\", \"text\")\n",
    "#Set to torch format\n",
    "mistral_tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "mistral_data_collator = DataCollatorWithPadding(tokenizer=mistral_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral\n",
    "A. Load checkpoints for the classfication model\n",
    "\n",
    "Let's load pre-trained Mistral 7B model with a sequence classification header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification # Load a pre-trained model with a sequence classification header \n",
    "import torch\n",
    "mistral_model =  AutoModelForSequenceClassification.from_pretrained(\n",
    "  pretrained_model_name_or_path=mistral_checkpoint,\n",
    "  num_labels=2,\n",
    "  device_map=\"auto\"\n",
    ")\n",
    "\n",
    "#For Mistral 7B, we have to add the padding token id as it is not defined by default.\n",
    "\n",
    "mistral_model.config.pad_token_id = mistral_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. LoRa setup for Mistral 7B classifier\n",
    "\n",
    "For Mistral 7B model, we need to specify the target_modules (The query and value vectors from the attention modules):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "mistral_peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=2, lora_alpha=16, lora_dropout=0.1, bias=\"none\", \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "mistral_model = get_peft_model(mistral_model, mistral_peft_config)\n",
    "mistral_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the trainer\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "First, we define the perfomance metrics we will use to compare the three models: F1 score, recall, precision and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#Define evaluation metrics\n",
    "import evaluate\n",
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    # All metrics are already predefined in the HF `evaluate` package\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric= evaluate.load(\"f1\")\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    # The trainer is expecting a dictionary where the keys are the metrics names and the values are the scores. \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Trainer for Weighted Loss\n",
    "\n",
    "As mentioned at the begining of this post, we have an imbalanced distribution between positive and negative classes. To account for that, we need to train our models with a weight cross-entropy loss. The Trainer class doesn't support providing a custom loss as it expects getting the loss directly from the model's outputs.\n",
    "\n",
    "So, we need to define our custom WeightedCELossTrainer that overrides the compute_loss method to calculate the weighted cross-entropy loss based on the model's predictions and the input labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "class WeightedCELossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # Get model's predictions\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([neg_weights, pos_weights], device=model.device, dtype=logits.dtype))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Setup\n",
    "\n",
    "Let's set the training arguments and the trainer for the three models.\n",
    "\n",
    "### B. Mistral 7B\n",
    "\n",
    "First important step is to move the models to the GPU device for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "mistral_model = mistral_model.cuda()\n",
    "\n",
    "lr = 1e-4\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"mistral-lora-token-classification\",\n",
    "    learning_rate=lr,\n",
    "    lr_scheduler_type= \"constant\",\n",
    "    warmup_ratio= 0.1,\n",
    "    max_grad_norm= 0.3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.001,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "\n",
    "mistral_trainer = WeightedBCELossTrainer(\n",
    "    model=mistral_model,\n",
    "    args=training_args,\n",
    "    train_dataset=mistral_tokenized_datasets['train'],\n",
    "    eval_dataset=mistral_tokenized_datasets[\"val\"],\n",
    "    data_collator=mistral_data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
